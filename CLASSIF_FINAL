library(caret) # necessaire pour la validation croisee stratifiee et le cv
library(tidyverse)
library(pROC)
library(gbm)
library(ranger)
library(glmnet)
library(bestglm)

data(SAheart)

don=SAheart %>% 
  rename(Y=chd) %>% 
  mutate(Y=as.factor(Y))


# Division en Apprentissage et Test
split=0.9

trainSize=round(0.9*nrow(SAheart))
A = sample(nrow(don), trainSize)
donApp  = don[A,]
donTest = don[-A,]

# Division en Apprentissage et Validation

nbloc=5

folds = createFolds(donApp$Y,
                    k=nbloc,
                    list=F)

# Pre-processing
donAppX = model.matrix(Y~., data=donApp)
donY = donApp$Y

# ==============================================================================
# Grilles
# ==============================================================================

# Foret
gr.foret=expand.grid(num.trees=c(100,600),
                     max.depth=c(1,3,5),
                     min.buckets=c(1,5),
                     mtry=c(1,2,5))
gr.foret.params=data.frame(gr.foret,'auc'=NA)

# SVM
gr.poly=expand.grid(C=c(0.1,10,100),
                    degree=c(1,2,3),
                    scale=1)
gr.radial=expand.grid(C=c(0.1,1,10),
                      sigma = c(0.0001,0.001,0.01,0.1,1))
ctrl = trainControl(method='cv', number=3)


# Validation croisee
SCORE=data.frame('Y'=donApp$Y,'logistic'=NA,'aic'=NA,'bic'=NA,'ridge'=NA,'lasso'=NA,'elnet'=NA,'foret'=NA,'pol_svm'=NA,'rad_svm'=NA)
SCORE %>% head()
SCORE %>% dim()

jj=2

for(jj in 1:nbloc){
  
  cat('Fold: ', jj, '\n')
  
  donA=donApp[folds!=jj,]
  donV=donApp[folds==jj,]

  donXA=donAppX[folds!=jj,]
  donXV=donAppX[folds==jj,]  
  donYA=donY[folds!=jj]

  
  # Logistique =================================================================
  logistic=glm(Y~., data=donA, family='binomial')
  SCORE[folds==jj,'logistic'] = predict(logistic,newdata=donV,type='response')
  
  # AIC ========================================================================
  aic=step(logistic,trace=0)
  SCORE[folds==jj,'aic']=predict(aic,newdata=donV,type='response')
  
  # BIC ========================================================================
  bic=step(logistic,trace=0,k=log(nrow(donA)))
  SCORE[folds==jj,'bic']=predict(bic,newdata=donV,type='response')
  
  # Penalisation =================================================================
  ridge=cv.glmnet(donXA,donYA,alpha=0  ,family='binomial',nfolds=5,type.measure='auc')
  lasso=cv.glmnet(donXA,donYA,alpha=1  ,family='binomial',nfolds=5,type.measure='auc')
  elnet=cv.glmnet(donXA,donYA,alpha=0.5,family='binomial',nfolds=5,type.measure='auc')
  SCORE[folds==jj,'ridge']=predict(ridge,newx=donXV,type='response',s='lambda.min')
  SCORE[folds==jj,'lasso']=predict(lasso,newx=donXV,type='response',s='lambda.min')
  SCORE[folds==jj,'elnet']=predict(elnet,newx=donXV,type='response',s='lambda.min')
  
  # Foret ======================================================================
  
  ### Hyper-parametrage
  for(xx in 1:nrow(gr.foret)){
    foret = ranger(factor(Y)~., data=donA,
                   classification = T,
                   probability = T,
                   num.trees = gr.foret$num.trees[xx],
                   max.depth = gr.foret$max.depth[xx])
    gr.foret.results = predict(foret,donV,type='response')
    gr.foret.params[xx,'auc'] = roc(donV$Y,gr.foret.results$predictions[,'1'],quiet=T)$auc
  }
  
  
  
  ### Selection du meilleur parametrage
  best.foret.params = gr.foret.params[which.max(gr.foret.params$auc),]
  best.foret = ranger(factor(Y)~., data=donA,
                      classification=T, probability = T,
                      num.trees=best.foret.params$num.trees,
                      max.depth = best.foret.params$max.depth,
                      min.bucket = best.foret.params$min.buckets,
                      mtry = best.foret.params$mtry)
  SCORE[folds==jj, 'foret']=predict(best.foret, donV, type='response')$predictions[,'1']
  
  # SVM ========================================================================
  svm.poly = train(Y~., data=donA, method = 'svmPoly', trControl = ctrl, tuneGrid = gr.poly)
  bestPoly=svm.poly$results[which.max(svm.poly$results$Accuracy),]
  tmpPol=ksvm(Y~., data=donA, kernel = 'polydot', kpar=list(degree=bestPoly$degree,scale=1,offset=1),C=bestPoly$C,prob.model=T)
  
  svm.radial = train(Y~., data=donA, method = 'svmRadial', trControl = ctrl, tuneGrid = gr.radial)
  bestRadial=svm.radial$results[which.max(svm.radial$results$Accuracy),]
  tmpRad=ksvm(Y~., data=donA, kernel =  'rbfdot', kpar=list(sigma=bestRadial$sigma),C=bestRadial$C,prob.model=T)
  
  SCORE[folds==jj,'pol_svm'] = predict(tmpPol, newdata=donV, type = 'prob')[,'1'] # rajouter l'index de colonne : il ne faut en prendre qu'une
  SCORE[folds==jj,'rad_svm'] = predict(tmpRad, newdata=donV, type= 'prob')[,'1']  
  
}


rocCV = roc(factor(Y)~., data=SCORE, quiet=T)

aucmodele = sort(round(unlist(lapply(rocCV,auc)),5),dec=TRUE)

tmp = lapply(rocCV, FUN=coords, x = 'best', 
             ret = c('threshold','tp','fp','fn','tn','sensitivity','specificity','ac'),
             transpose=T)

mat=do.call(rbind,tmp)

aucmodele
mat





